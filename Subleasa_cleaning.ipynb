{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3163d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL LISTINGS\n",
    "\n",
    "COLLEGE_LISTINGS = ['FIU.json', 'UGA.json', 'FSU.json', 'GSU.json' , 'GT.json', 'KSU.json']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a281b71",
   "metadata": {},
   "source": [
    "Actually Cleaning the Uploaded Listings to Key only the relevant Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b088a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered JSON saved to filtered_output.json\n",
      "Filtered JSON saved to filtered_output.json\n",
      "Filtered JSON saved to filtered_output.json\n",
      "Filtered JSON saved to filtered_output.json\n",
      "Filtered JSON saved to filtered_output.json\n",
      "Filtered JSON saved to filtered_output.json\n"
     ]
    }
   ],
   "source": [
    "from cleaning_utils import clean_college_lisitings\n",
    "for name in COLLEGE_LISTINGS:\n",
    "    collegeName = name.split('.')[0]\n",
    "    clean_college_lisitings(collegeName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0258e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name dataset_facebook-groups-scraper_2025-05-24_12-04-37-308.json\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define your keyword filters\n",
    "KEYWORDS = ['sublease', 'subleasing', 'roommate', 'roommates', 'apartment', 'apartments', 'rent', 'room']\n",
    "\n",
    "def post_matches(text, keywords):\n",
    "    \"\"\"Return True if any keyword is found in the text (case-insensitive).\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return any(kw in text_lower for kw in keywords)\n",
    "\n",
    "def filter_listings(input_json_path, output_json_path):\n",
    "    # Load the data\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        listings = json.load(f)\n",
    "\n",
    "    filtered = []\n",
    "    # Iterate with a tqdm progress bar\n",
    "    for post in tqdm(listings, desc=\"Filtering posts\"):\n",
    "        # Assume each post has a 'message' or 'text' field\n",
    "        text = post.get('message') or post.get('text') or ''\n",
    "        if post_matches(text, KEYWORDS):\n",
    "            filtered.append(post)\n",
    "\n",
    "    # Save the filtered results\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nFound {len(filtered)} matching posts. Saved to {output_json_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ade288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering posts: 100%|██████████| 200/200 [00:00<00:00, 219654.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 170 matching posts. Saved to actualSubleases/FIU_actual_sublease_output.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering posts: 100%|██████████| 200/200 [00:00<00:00, 61016.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 99 matching posts. Saved to actualSubleases/UGA_actual_sublease_output.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering posts: 100%|██████████| 200/200 [00:00<00:00, 129513.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 174 matching posts. Saved to actualSubleases/FSU_actual_sublease_output.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering posts: 100%|██████████| 200/200 [00:00<00:00, 151912.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 153 matching posts. Saved to actualSubleases/GSU_actual_sublease_output.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering posts: 100%|██████████| 200/200 [00:00<00:00, 113528.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 129 matching posts. Saved to actualSubleases/GT_actual_sublease_output.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering posts: 100%|██████████| 200/200 [00:00<00:00, 66156.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 181 matching posts. Saved to actualSubleases/KSU_actual_sublease_output.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#INPUT_FILE = \"removed_irrelevant_keys_output.json\"\n",
    "\n",
    "for input in COLLEGE_LISTINGS:\n",
    "    file_dir = f\"dataFiles/{input}\"\n",
    "    output_name = f\"actualSubleases/{input.split('.')[0]}_actual_sublease_output.json\"\n",
    "    filter_listings(file_dir, output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59debc0c",
   "metadata": {},
   "source": [
    "After we have sieved out all irrelevant listings which dont have to do with apartments, time to Upload the images to firestore then remove the irrelevant fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc1e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "API_BASE_URL = \"http://localhost:5000\"\n",
    "\n",
    "def test_from_file_json(inputFile, outputFile):\n",
    "    # Charger le fichier JSON\n",
    "    with open(inputFile, 'r', encoding='utf-8') as f:\n",
    "        listings = json.load(f)\n",
    "\n",
    "    # Envoyer à l’API Flask\n",
    "    response = requests.post(f'{API_BASE_URL}/upload-images', json=listings)\n",
    "\n",
    "    # Vérifier la réponse\n",
    "    if response.ok:\n",
    "        result = response.json()\n",
    "        print(\"✅ Traitement réussi :\", result['message'])\n",
    "        \n",
    "        # Sauvegarder la nouvelle version avec les images mises à jour\n",
    "        with open(outputFile, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result['listings'], f, indent=2, ensure_ascii=False)\n",
    "        print(\"✅ Fichier output.json généré avec succès\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ Erreur :\", response.status_code, response.text)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60a9404b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Traitement réussi : Processed 153 listings. 153 successful, 0 failed. 803 images downloaded.\n",
      "✅ Fichier output.json généré avec succès\n",
      "✅ Traitement réussi : Processed 129 listings. 129 successful, 0 failed. 623 images downloaded.\n",
      "✅ Fichier output.json généré avec succès\n",
      "✅ Traitement réussi : Processed 181 listings. 181 successful, 0 failed. 325 images downloaded.\n",
      "✅ Fichier output.json généré avec succès\n"
     ]
    }
   ],
   "source": [
    "SUB_COLLEGE = COLLEGE_LISTINGS[-3:]\n",
    "for input in SUB_COLLEGE:\n",
    "    file_dir = f\"actualSubleases/{input.split('.')[0]}_actual_sublease_output.json\"\n",
    "    output_name = f\"uploadFiles/{input.split('.')[0]}_uploaded.json\"\n",
    "    test_from_file_json(file_dir, output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c87e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting non-matches: 100%|██████████| 20/20 [00:00<00:00, 118650.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 7 non-matching posts to new_non_matching.json\n",
      "Total non-matching posts: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def collect_non_matching(input_json_path, output_json_path=None):\n",
    "    \"\"\"Return a list of posts that do NOT contain any of the KEYWORDS.\n",
    "       Optionally save them out to a JSON file if output_json_path is given.\"\"\"\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        listings = json.load(f)\n",
    "\n",
    "    non_matching = []\n",
    "    for post in tqdm(listings, desc=\"Collecting non-matches\"):\n",
    "        text = post.get('message') or post.get('text') or ''\n",
    "        if not post_matches(text, KEYWORDS):\n",
    "            non_matching.append(post)\n",
    "\n",
    "    if output_json_path:\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(non_matching, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved {len(non_matching)} non-matching posts to {output_json_path}\")\n",
    "\n",
    "    return non_matching\n",
    "# This is kinda a Sanity check cause the sum of its values plus the ones above have to give the total number of initial Listings\n",
    "if __name__ == \"__main__\":\n",
    "    non_matches = collect_non_matching(\"filtered_output.json\", \"new_non_matching.json\")\n",
    "    print(f\"Total non-matching posts: {len(non_matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6284a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering posts: 100%|██████████| 7/7 [00:00<00:00, 69245.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 matching posts. Saved to filtered_nonmatching_subleases.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#This is just a Sanity check, it should always be 0 if everything was done right\n",
    "if __name__ == \"__main__\":\n",
    "    filter_listings(\"new_non_matching.json\", \"filtered_nonmatching_subleases.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285914cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
